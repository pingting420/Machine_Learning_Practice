{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first step in this competition was analyzing Last year's WIDS competition and after some EDA and comparing Data in hope of using 2020's data for augmenting 2021's, I reached to a conclusion that might be obvious for last year's participants. WIDS 2021 Train data is the combination of WIDS 2020's Train and Test Data with some features dropped. So the simplest path forward was checking solutions and features from last year's top solutions. My base for this notebook is [Kain's work (5 place)](https://www.kaggle.com/kainsama/single-lgbm-v0-1-0) and [Dan Ofer's works (1 place)](https://www.kaggle.com/danofer/wids-2020-competitive-1st-place-component). Also Features are borrowed from [jayjay75's work (3 place)](https://www.kaggle.com/jayjay75/3rd-place-nn-wids2020?scriptVersionId=29209297) and some ideas are based on [dynamic24's write-up (6 place)](https://www.kaggle.com/c/widsdatathon2020/discussion/133509)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE based on last year (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-10T13:06:36.627567Z",
     "iopub.status.busy": "2021-02-10T13:06:36.626767Z",
     "iopub.status.idle": "2021-02-10T13:06:38.666103Z",
     "shell.execute_reply": "2021-02-10T13:06:38.664789Z"
    },
    "papermill": {
     "duration": 2.109011,
     "end_time": "2021-02-10T13:06:38.666417",
     "exception": false,
     "start_time": "2021-02-10T13:06:36.557406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, iqr, skew, gmean, hmean, mode, normaltest, shapiro, ks_2samp\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO, ERROR\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os, gc, sys, time, random, math\n",
    "from contextlib import contextmanager\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy import stats, special\n",
    "from sklearn import set_config\n",
    "from functools import partial\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import typing as tp\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger():\n",
    "    handler = StreamHandler()\n",
    "    handler.setLevel(ERROR)\n",
    "    handler.setFormatter(Formatter(LOGFORMAT))\n",
    "    fh_handler = FileHandler('{}.log'.format(MODELNAME))\n",
    "    fh_handler.setFormatter(Formatter(LOGFORMAT))\n",
    "    logger.setLevel(ERROR)\n",
    "    logger.addHandler(handler)\n",
    "    logger.addHandler(fh_handler)\n",
    "    \n",
    "@contextmanager\n",
    "def timer(name : tp.Text):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "COMPETITION = 'WIDS2021'\n",
    "logger = getLogger(COMPETITION)\n",
    "LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
    "MODELNAME = 'LGBMCV'\n",
    "init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-02-10T13:06:38.772036Z",
     "iopub.status.busy": "2021-02-10T13:06:38.771306Z",
     "iopub.status.idle": "2021-02-10T13:06:43.723425Z",
     "shell.execute_reply": "2021-02-10T13:06:43.722523Z"
    },
    "papermill": {
     "duration": 5.008628,
     "end_time": "2021-02-10T13:06:43.723641",
     "exception": false,
     "start_time": "2021-02-10T13:06:38.715013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/widsdatathon2021/TrainingWiDS2021.csv', index_col=[0])\n",
    "test = pd.read_csv('../input/widsdatathon2021/UnlabeledWiDS2021.csv', index_col=[0])\n",
    "test_id = test.encounter_id.values\n",
    "y = train.diabetes_mellitus.values\n",
    "del train['diabetes_mellitus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\n",
    "test = test.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\n",
    "train.loc[train.age == 0, 'age'] = np.nan\n",
    "train = train.drop(['readmission_status','encounter_id','hospital_id'], axis=1)\n",
    "test = test.drop(['readmission_status','encounter_id','hospital_id'], axis=1)\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_feats=[f[:-4] for f in train.columns if f[-4:]=='_min']\n",
    "# detect if any of the min-max values are input wrongly\n",
    "train_count = dict()\n",
    "test_count = dict()\n",
    "for col in min_max_feats:\n",
    "    train.loc[train[f'{col}_min'] > train[f'{col}_max'], [f'{col}_min', f'{col}_max']] = train.loc[train[f'{col}_min'] > train[f'{col}_max'], [f'{col}_max', f'{col}_min']].values\n",
    "    test.loc[test[f'{col}_min'] > test[f'{col}_max'], [f'{col}_min', f'{col}_max']] = test.loc[test[f'{col}_min'] > test[f'{col}_max'], [f'{col}_max', f'{col}_min']].values\n",
    "    if (np.any(train[f'{col}_min'] > train[f'{col}_max'])):\n",
    "        train_count[col] = np.sum(train[f'{col}_min'] > train[f'{col}_max'])\n",
    "    if (np.any(test[f'{col}_min'] > test[f'{col}_max'])):\n",
    "        test_count[col] = np.sum(train[f'{col}_min'] > train[f'{col}_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:06:44.370757Z",
     "iopub.status.busy": "2021-02-10T13:06:44.365615Z",
     "iopub.status.idle": "2021-02-10T13:06:44.617628Z",
     "shell.execute_reply": "2021-02-10T13:06:44.61703Z"
    },
    "papermill": {
     "duration": 0.341099,
     "end_time": "2021-02-10T13:06:44.617804",
     "exception": false,
     "start_time": "2021-02-10T13:06:44.276705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(train.head(2))\n",
    "    display(train.describe())\n",
    "    display(test.head(2))\n",
    "    display(test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:06:44.735009Z",
     "iopub.status.busy": "2021-02-10T13:06:44.732738Z",
     "iopub.status.idle": "2021-02-10T13:06:44.867415Z",
     "shell.execute_reply": "2021-02-10T13:06:44.866439Z"
    },
    "papermill": {
     "duration": 0.195646,
     "end_time": "2021-02-10T13:06:44.867708",
     "exception": false,
     "start_time": "2021-02-10T13:06:44.672062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lbls = {}\n",
    "# use encoder to change all the categorical data into str type\n",
    "for col in train.select_dtypes(exclude = np.number).columns.tolist():\n",
    "    le = LabelEncoder().fit(pd.concat([train[col].astype(str),test[col].astype(str)]))   \n",
    "    train[col] = le.transform(train[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "    lbls[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminary feature engineering\n",
    "# maybe to rescale the feature here and filled na values with 0\n",
    "train['comorbidity_score'] = train['aids'].values * 23 + train['cirrhosis'] * 4  + train['hepatic_failure'] * 16 + train['immunosuppression'] * 10 + train['leukemia'] * 10 + train['lymphoma'] * 13 + train['solid_tumor_with_metastasis'] * 11\n",
    "test['comorbidity_score'] = test['aids'].values * 23 + test['cirrhosis'] * 4  + test['hepatic_failure'] * 16 + test['immunosuppression'] * 10 + test['leukemia'] * 10 + test['lymphoma'] * 13 + test['solid_tumor_with_metastasis'] * 11\n",
    "train['comorbidity_score'] = train['comorbidity_score'].fillna(-1)\n",
    "test['comorbidity_score'] = test['comorbidity_score'].fillna(-1)\n",
    "train['gcs_sum'] = train['gcs_eyes_apache']+train['gcs_motor_apache']+train['gcs_verbal_apache']\n",
    "test['gcs_sum'] = test['gcs_eyes_apache']+test['gcs_motor_apache']+test['gcs_verbal_apache']\n",
    "train['gcs_sum'] = train['gcs_sum'].fillna(0)\n",
    "test['gcs_sum'] = test['gcs_sum'].fillna(0)\n",
    "train['apache_2_diagnosis_type'] = train.apache_2_diagnosis.round(-1).fillna(-100).astype('int32')\n",
    "test['apache_2_diagnosis_type'] = test.apache_2_diagnosis.round(-1).fillna(-100).astype('int32')\n",
    "train['apache_3j_diagnosis_type'] = train.apache_3j_diagnosis.round(-2).fillna(-100).astype('int32')\n",
    "test['apache_3j_diagnosis_type'] = test.apache_3j_diagnosis.round(-2).fillna(-100).astype('int32')\n",
    "train['bmi_type'] = train.bmi.fillna(0).apply(lambda x: 5 * (round(int(x)/5)))\n",
    "test['bmi_type'] = test.bmi.fillna(0).apply(lambda x: 5 * (round(int(x)/5)))\n",
    "train['height_type'] = train.height.fillna(0).apply(lambda x: 5 * (round(int(x)/5)))\n",
    "test['height_type'] = test.height.fillna(0).apply(lambda x: 5 * (round(int(x)/5)))\n",
    "train['weight_type'] = train.weight.fillna(0).apply(lambda x: 5 * (round(int(x)/5)))\n",
    "test['weight_type'] = test.weight.fillna(0).apply(lambda x: 5 * (round(int(x)/5)))\n",
    "train['age_type'] = train.age.fillna(0).apply(lambda x: 10 * (round(int(x)/10)))\n",
    "test['age_type'] = test.age.fillna(0).apply(lambda x: 10 * (round(int(x)/10)))\n",
    "train['gcs_sum_type'] = train.gcs_sum.fillna(0).apply(lambda x: 2.5 * (round(int(x)/2.5))).divide(2.5)\n",
    "test['gcs_sum_type'] = test.gcs_sum.fillna(0).apply(lambda x: 2.5 * (round(int(x)/2.5))).divide(2.5)\n",
    "train['apache_3j_diagnosis_x'] = train['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\n",
    "train['apache_2_diagnosis_x'] = train['apache_2_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\n",
    "test['apache_3j_diagnosis_x'] = test['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\n",
    "test['apache_2_diagnosis_x'] = test['apache_2_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\n",
    "train['apache_3j_diagnosis_split1'] = np.where(train['apache_3j_diagnosis'].isna() , np.nan , train['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[1]  )\n",
    "test['apache_3j_diagnosis_split1']  = np.where(test['apache_3j_diagnosis'].isna() , np.nan , test['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[1]  )\n",
    "train['apache_2_diagnosis_split1'] = np.where(train['apache_2_diagnosis'].isna() , np.nan , train['apache_2_diagnosis'].apply(lambda x : x % 10)  )\n",
    "test['apache_2_diagnosis_split1']  = np.where(test['apache_2_diagnosis'].isna() , np.nan , test['apache_2_diagnosis'].apply(lambda x : x % 10) )\n",
    "\n",
    "\n",
    "# Use these five columns as identifying columns\n",
    "IDENTIFYING_COLS = ['age_type', 'height_type',  'ethnicity', 'gender', 'bmi_type'] \n",
    "train['profile'] = train[IDENTIFYING_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\n",
    "test['profile'] = test[IDENTIFYING_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\n",
    "print(f'Number of unique Profiles : {train[\"profile\"].nunique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comorbidity_score'] = train['aids'].values * 23 + train['cirrhosis'] * 4  + train['hepatic_failure'] * 16 + train['immunosuppression'] * 10 + train['leukemia'] * 10 + train['lymphoma'] * 13 + train['solid_tumor_with_metastasis'] * 11\n",
    "test['comorbidity_score'] = test['aids'].values * 23 + test['cirrhosis'] * 4  + test['hepatic_failure'] * 16 + test['immunosuppression'] * 10 + test['leukemia'] * 10 + test['lymphoma'] * 13 + test['solid_tumor_with_metastasis'] * 11\n",
    "train['comorbidity_score'] = train['comorbidity_score'].fillna(-1)\n",
    "test['comorbidity_score'] = test['comorbidity_score'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:06:48.381824Z",
     "iopub.status.busy": "2021-02-10T13:06:48.381055Z",
     "iopub.status.idle": "2021-02-10T13:06:48.414494Z",
     "shell.execute_reply": "2021-02-10T13:06:48.413795Z"
    },
    "papermill": {
     "duration": 0.097272,
     "end_time": "2021-02-10T13:06:48.414671",
     "exception": false,
     "start_time": "2021-02-10T13:06:48.317399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generated icu_id_counts with log1p\n",
    "# not sure the use of df here\n",
    "df = pd.concat([train['icu_id'], test['icu_id']])\n",
    "agg = df.value_counts().to_dict()\n",
    "train['icu_id_counts'] = np.log1p(train['icu_id'].map(agg))\n",
    "test['icu_id_counts'] = np.log1p(test['icu_id'].map(agg))\n",
    "# not sure the use of df here\n",
    "df = pd.concat([train['age'], test['age']])\n",
    "agg = df.value_counts().to_dict()\n",
    "# applied log1p on age\n",
    "train['age_counts'] = np.log1p(train['age'].map(agg))\n",
    "test['age_counts'] = np.log1p(test['age'].map(agg))\n",
    "# seems to be testing the calculation of bmi\n",
    "train[\"diff_bmi\"] = train['bmi'].copy() \n",
    "train['bmi'] = train['weight']/((train['height']/100)**2)\n",
    "train[\"diff_bmi\"] = train[\"diff_bmi\"]-train['bmi']\n",
    "test[\"diff_bmi\"] = test['bmi'].copy()\n",
    "test['bmi'] = test['weight']/((test['height']/100)**2)\n",
    "test[\"diff_bmi\"] = test[\"diff_bmi\"]-test['bmi']\n",
    "# took inverse of the logit function on pre-icu-los-days\n",
    "train['pre_icu_los_days'] = train['pre_icu_los_days'].apply(lambda x:special.expit(x) )\n",
    "test['pre_icu_los_days']  = test['pre_icu_los_days'].apply(lambda x:special.expit(x) )\n",
    "# new features, combination of age and bmi\n",
    "train['abmi'] = train['age']/train['bmi']\n",
    "train['agi'] = train['weight']/train['age']\n",
    "test['abmi'] = test['age']/train['bmi']\n",
    "test['agi'] = test['weight']/train['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:06:48.785186Z",
     "iopub.status.busy": "2021-02-10T13:06:48.784146Z",
     "iopub.status.idle": "2021-02-10T13:06:49.190754Z",
     "shell.execute_reply": "2021-02-10T13:06:49.189993Z"
    },
    "papermill": {
     "duration": 0.471947,
     "end_time": "2021-02-10T13:06:49.190966",
     "exception": false,
     "start_time": "2021-02-10T13:06:48.719019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Interesting\n",
    "# d1 means daily lab\n",
    "# h1 means hourly lab\n",
    "d_cols = [c for c in train.columns if(c.startswith(\"d1\"))]\n",
    "h_cols = [c for c in train.columns if(c.startswith(\"h1\"))]\n",
    "train[\"dailyLabs_row_nan_count\"] = train[d_cols].isna().sum(axis=1)\n",
    "train[\"hourlyLabs_row_nan_count\"] = train[h_cols].isna().sum(axis=1)\n",
    "train[\"diff_labTestsRun_daily_hourly\"] = train[\"dailyLabs_row_nan_count\"] - train[\"hourlyLabs_row_nan_count\"]\n",
    "test[\"dailyLabs_row_nan_count\"] = test[d_cols].isna().sum(axis=1)\n",
    "test[\"hourlyLabs_row_nan_count\"] = test[h_cols].isna().sum(axis=1)\n",
    "test[\"diff_labTestsRun_daily_hourly\"] = test[\"dailyLabs_row_nan_count\"] - test[\"hourlyLabs_row_nan_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:06:49.318397Z",
     "iopub.status.busy": "2021-02-10T13:06:49.31597Z",
     "iopub.status.idle": "2021-02-10T13:06:50.185245Z",
     "shell.execute_reply": "2021-02-10T13:06:50.186444Z"
    },
    "papermill": {
     "duration": 0.938288,
     "end_time": "2021-02-10T13:06:50.186712",
     "exception": false,
     "start_time": "2021-02-10T13:06:49.248424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print out all the lab names, regardless if daily or hourly lab tests\n",
    "lab_col = [c for c in train.columns if((c.startswith(\"h1\")) | (c.startswith(\"d1\")))]\n",
    "lab_col_names = list(set(list(map(lambda i: i[ 3 : -4], lab_col))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:06:50.576935Z",
     "iopub.status.busy": "2021-02-10T13:06:50.568532Z",
     "iopub.status.idle": "2021-02-10T13:08:37.367961Z",
     "shell.execute_reply": "2021-02-10T13:08:37.368544Z"
    },
    "papermill": {
     "duration": 106.885134,
     "end_time": "2021-02-10T13:08:37.368774",
     "exception": false,
     "start_time": "2021-02-10T13:06:50.48364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare the features between daily lab and hourly lab tests\n",
    "first_h = []\n",
    "for v in lab_col_names:\n",
    "    first_h.append(v+\"_started_after_firstHour\")\n",
    "    colsx = [x for x in test.columns if v in x]\n",
    "    train[v+\"_nans\"] = train.loc[:, colsx].isna().sum(axis=1)\n",
    "    test[v+\"_nans\"] = test.loc[:, colsx].isna().sum(axis=1)\n",
    "    train[v+\"_d1_value_range\"] = train[f\"d1_{v}_max\"].subtract(train[f\"d1_{v}_min\"])    \n",
    "    train[v+\"_h1_value_range\"] = train[f\"h1_{v}_max\"].subtract(train[f\"h1_{v}_min\"])\n",
    "    train[v+\"_d1_h1_max_eq\"] = (train[f\"d1_{v}_max\"]== train[f\"h1_{v}_max\"]).astype(np.int8)\n",
    "    train[v+\"_d1_h1_min_eq\"] = (train[f\"d1_{v}_min\"]== train[f\"h1_{v}_min\"]).astype(np.int8)\n",
    "    train[v+\"_d1_zero_range\"] = (train[v+\"_d1_value_range\"] == 0).astype(np.int8)\n",
    "    train[v+\"_h1_zero_range\"] =(train[v+\"_h1_value_range\"] == 0).astype(np.int8)\n",
    "    train[v+\"_tot_change_value_range_normed\"] = abs((train[v+\"_d1_value_range\"].div(train[v+\"_h1_value_range\"])))#.div(df[f\"d1_{v}_max\"]))\n",
    "    train[v+\"_started_after_firstHour\"] = ((train[f\"h1_{v}_max\"].isna()) & (train[f\"h1_{v}_min\"].isna())) & (~train[f\"d1_{v}_max\"].isna())\n",
    "    train[v+\"_day_more_extreme\"] = ((train[f\"d1_{v}_max\"]>train[f\"h1_{v}_max\"]) | (train[f\"d1_{v}_min\"]<train[f\"h1_{v}_min\"]))\n",
    "    train[v+\"_day_more_extreme\"].fillna(False)    \n",
    "    test[v+\"_d1_value_range\"] = test[f\"d1_{v}_max\"].subtract(test[f\"d1_{v}_min\"])   \n",
    "    test[v+\"_h1_value_range\"] = test[f\"h1_{v}_max\"].subtract(test[f\"h1_{v}_min\"])\n",
    "    test[v+\"_d1_h1_max_eq\"] = (test[f\"d1_{v}_max\"]== test[f\"h1_{v}_max\"]).astype(np.int8)\n",
    "    test[v+\"_d1_h1_min_eq\"] = (test[f\"d1_{v}_min\"]== test[f\"h1_{v}_min\"]).astype(np.int8)\n",
    "    test[v+\"_d1_zero_range\"] = (test[v+\"_d1_value_range\"] == 0).astype(np.int8)\n",
    "    test[v+\"_h1_zero_range\"] =(test[v+\"_h1_value_range\"] == 0).astype(np.int8)\n",
    "    test[v+\"_tot_change_value_range_normed\"] = abs((test[v+\"_d1_value_range\"].div(test[v+\"_h1_value_range\"])))\n",
    "    test[v+\"_started_after_firstHour\"] = ((test[f\"h1_{v}_max\"].isna()) & (test[f\"h1_{v}_min\"].isna())) & (~test[f\"d1_{v}_max\"].isna())\n",
    "    test[v+\"_day_more_extreme\"] = ((test[f\"d1_{v}_max\"]>test[f\"h1_{v}_max\"]) | (test[f\"d1_{v}_min\"]<test[f\"h1_{v}_min\"]))\n",
    "    test[v+\"_day_more_extreme\"].fillna(False)\n",
    "\n",
    "train[\"total_Tests_started_After_firstHour\"] = train[first_h].sum(axis=1)\n",
    "test[\"total_Tests_started_After_firstHour\"] = test[first_h].sum(axis=1)\n",
    "gc.collect()\n",
    "train[\"total_Tests_started_After_firstHour\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupers = ['apache_3j_diagnosis', 'profile']\n",
    "\n",
    "for g in groupers:\n",
    "    for v in lab_col_names:\n",
    "        temp = pd.concat([train[[f\"d1_{v}_max\",g]], test[[f\"d1_{v}_max\",g]]], axis=0).groupby(g)[f\"d1_{v}_max\"].mean().to_dict()\n",
    "        train[f'mean_diff_d1_{v}_{g}_max'] = train[f\"d1_{v}_max\"]-train[g].map(temp)\n",
    "        test[f'mean_diff_d1_{v}_{g}_max'] = test[f\"d1_{v}_max\"]-test[g].map(temp)\n",
    "        temp = pd.concat([train[[f\"d1_{v}_min\",g]], test[[f\"d1_{v}_min\",g]]], axis=0).groupby(g)[f\"d1_{v}_min\"].mean().to_dict()   \n",
    "        train[f'mean_diff_d1_{v}_{g}_min'] = train[f\"d1_{v}_min\"]-train[g].map(temp)\n",
    "        test[f'mean_diff_d1_{v}_{g}_min'] = test[f\"d1_{v}_min\"]-test[g].map(temp)\n",
    "        temp = pd.concat([train[[f\"h1_{v}_max\",g]], test[[f\"h1_{v}_max\",g]]], axis=0).groupby(g)[f\"h1_{v}_max\"].mean().to_dict()   \n",
    "        train[f'mean_diff_h1_{v}_{g}_max'] = train[f\"h1_{v}_max\"]-train[g].map(temp)\n",
    "        test[f'mean_diff_h1_{v}_{g}_max'] = test[f\"h1_{v}_max\"]-test[g].map(temp)\n",
    "        temp = pd.concat([train[[f\"h1_{v}_min\",g]], test[[f\"h1_{v}_min\",g]]], axis=0).groupby(g)[f\"h1_{v}_min\"].mean().to_dict()   \n",
    "        train[f'mean_diff_h1_{v}_{g}_min'] = train[f\"h1_{v}_min\"]-train[g].map(temp)\n",
    "        test[f'mean_diff_h1_{v}_{g}_min'] = test[f\"h1_{v}_min\"]-test[g].map(temp)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:08:37.660633Z",
     "iopub.status.busy": "2021-02-10T13:08:37.637381Z",
     "iopub.status.idle": "2021-02-10T13:08:38.220266Z",
     "shell.execute_reply": "2021-02-10T13:08:38.219503Z"
    },
    "papermill": {
     "duration": 0.794499,
     "end_time": "2021-02-10T13:08:38.220443",
     "exception": false,
     "start_time": "2021-02-10T13:08:37.425944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['diasbp_indicator'] = (\n",
    "(train['d1_diasbp_invasive_max'] == train['d1_diasbp_max']) & (train['d1_diasbp_noninvasive_max']==train['d1_diasbp_invasive_max'])|\n",
    "(train['d1_diasbp_invasive_min'] == train['d1_diasbp_min']) & (train['d1_diasbp_noninvasive_min']==train['d1_diasbp_invasive_min'])|\n",
    "(train['h1_diasbp_invasive_max'] == train['h1_diasbp_max']) & (train['h1_diasbp_noninvasive_max']==train['h1_diasbp_invasive_max'])|\n",
    "(train['h1_diasbp_invasive_min'] == train['h1_diasbp_min']) & (train['h1_diasbp_noninvasive_min']==train['h1_diasbp_invasive_min'])\n",
    ").astype(np.int8)\n",
    "\n",
    "\n",
    "train['mbp_indicator'] = (\n",
    "(train['d1_mbp_invasive_max'] == train['d1_mbp_max']) & (train['d1_mbp_noninvasive_max']==train['d1_mbp_invasive_max'])|\n",
    "(train['d1_mbp_invasive_min'] == train['d1_mbp_min']) & (train['d1_mbp_noninvasive_min']==train['d1_mbp_invasive_min'])|\n",
    "(train['h1_mbp_invasive_max'] == train['h1_mbp_max']) & (train['h1_mbp_noninvasive_max']==train['h1_mbp_invasive_max'])|\n",
    "(train['h1_mbp_invasive_min'] == train['h1_mbp_min']) & (train['h1_mbp_noninvasive_min']==train['h1_mbp_invasive_min'])\n",
    ").astype(np.int8)\n",
    "\n",
    "train['sysbp_indicator'] = (\n",
    "(train['d1_sysbp_invasive_max'] == train['d1_sysbp_max']) & (train['d1_sysbp_noninvasive_max']==train['d1_sysbp_invasive_max'])|\n",
    "(train['d1_sysbp_invasive_min'] == train['d1_sysbp_min']) & (train['d1_sysbp_noninvasive_min']==train['d1_sysbp_invasive_min'])|\n",
    " (train['h1_sysbp_invasive_max'] == train['h1_sysbp_max']) & (train['h1_sysbp_noninvasive_max']==train['h1_sysbp_invasive_max'])|\n",
    "(train['h1_sysbp_invasive_min'] == train['h1_sysbp_min']) & (train['h1_sysbp_noninvasive_min']==train['h1_sysbp_invasive_min'])   \n",
    ").astype(np.int8)\n",
    "\n",
    "train['d1_mbp_invnoninv_max_diff'] = train['d1_mbp_invasive_max'] - train['d1_mbp_noninvasive_max']\n",
    "train['h1_mbp_invnoninv_max_diff'] = train['h1_mbp_invasive_max'] - train['h1_mbp_noninvasive_max']\n",
    "train['d1_mbp_invnoninv_min_diff'] = train['d1_mbp_invasive_min'] - train['d1_mbp_noninvasive_min']\n",
    "train['h1_mbp_invnoninv_min_diff'] = train['h1_mbp_invasive_min'] - train['h1_mbp_noninvasive_min']\n",
    "train['d1_diasbp_invnoninv_max_diff'] = train['d1_diasbp_invasive_max'] - train['d1_diasbp_noninvasive_max']\n",
    "train['h1_diasbp_invnoninv_max_diff'] = train['h1_diasbp_invasive_max'] - train['h1_diasbp_noninvasive_max']\n",
    "train['d1_diasbp_invnoninv_min_diff'] = train['d1_diasbp_invasive_min'] - train['d1_diasbp_noninvasive_min']\n",
    "train['h1_diasbp_invnoninv_min_diff'] = train['h1_diasbp_invasive_min'] - train['h1_diasbp_noninvasive_min']\n",
    "train['d1_sysbp_invnoninv_max_diff'] = train['d1_sysbp_invasive_max'] - train['d1_sysbp_noninvasive_max']\n",
    "train['h1_sysbp_invnoninv_max_diff'] = train['h1_sysbp_invasive_max'] - train['h1_sysbp_noninvasive_max']\n",
    "train['d1_sysbp_invnoninv_min_diff'] = train['d1_sysbp_invasive_min'] - train['d1_sysbp_noninvasive_min']\n",
    "train['h1_sysbp_invnoninv_min_diff'] = train['h1_sysbp_invasive_min'] - train['h1_sysbp_noninvasive_min']\n",
    "\n",
    "test['diasbp_indicator'] = (\n",
    "(test['d1_diasbp_invasive_max'] == test['d1_diasbp_max']) & (test['d1_diasbp_noninvasive_max']==test['d1_diasbp_invasive_max'])|\n",
    "(test['d1_diasbp_invasive_min'] == test['d1_diasbp_min']) & (test['d1_diasbp_noninvasive_min']==test['d1_diasbp_invasive_min'])|\n",
    "(test['h1_diasbp_invasive_max'] == test['h1_diasbp_max']) & (test['h1_diasbp_noninvasive_max']==test['h1_diasbp_invasive_max'])|\n",
    "(test['h1_diasbp_invasive_min'] == test['h1_diasbp_min']) & (test['h1_diasbp_noninvasive_min']==test['h1_diasbp_invasive_min'])\n",
    ").astype(np.int8)\n",
    "\n",
    "\n",
    "test['mbp_indicator'] = (\n",
    "(test['d1_mbp_invasive_max'] == test['d1_mbp_max']) & (test['d1_mbp_noninvasive_max']==test['d1_mbp_invasive_max'])|\n",
    "(test['d1_mbp_invasive_min'] == test['d1_mbp_min']) & (test['d1_mbp_noninvasive_min']==test['d1_mbp_invasive_min'])|\n",
    "(test['h1_mbp_invasive_max'] == test['h1_mbp_max']) & (test['h1_mbp_noninvasive_max']==test['h1_mbp_invasive_max'])|\n",
    "(test['h1_mbp_invasive_min'] == test['h1_mbp_min']) & (test['h1_mbp_noninvasive_min']==test['h1_mbp_invasive_min'])\n",
    ").astype(np.int8)\n",
    "\n",
    "test['sysbp_indicator'] = (\n",
    "(test['d1_sysbp_invasive_max'] == test['d1_sysbp_max']) & (test['d1_sysbp_noninvasive_max']==test['d1_sysbp_invasive_max'])|\n",
    "(test['d1_sysbp_invasive_min'] == test['d1_sysbp_min']) & (test['d1_sysbp_noninvasive_min']==test['d1_sysbp_invasive_min'])|\n",
    " (test['h1_sysbp_invasive_max'] == test['h1_sysbp_max']) & (test['h1_sysbp_noninvasive_max']==test['h1_sysbp_invasive_max'])|\n",
    "(test['h1_sysbp_invasive_min'] == test['h1_sysbp_min']) & (test['h1_sysbp_noninvasive_min']==test['h1_sysbp_invasive_min'])   \n",
    ").astype(np.int8)\n",
    "\n",
    "test['d1_mbp_invnoninv_max_diff'] = test['d1_mbp_invasive_max'] - test['d1_mbp_noninvasive_max']\n",
    "test['h1_mbp_invnoninv_max_diff'] = test['h1_mbp_invasive_max'] - test['h1_mbp_noninvasive_max']\n",
    "test['d1_mbp_invnoninv_min_diff'] = test['d1_mbp_invasive_min'] - test['d1_mbp_noninvasive_min']\n",
    "test['h1_mbp_invnoninv_min_diff'] = test['h1_mbp_invasive_min'] - test['h1_mbp_noninvasive_min']\n",
    "test['d1_diasbp_invnoninv_max_diff'] = test['d1_diasbp_invasive_max'] - test['d1_diasbp_noninvasive_max']\n",
    "test['h1_diasbp_invnoninv_max_diff'] = test['h1_diasbp_invasive_max'] - test['h1_diasbp_noninvasive_max']\n",
    "test['d1_diasbp_invnoninv_min_diff'] = test['d1_diasbp_invasive_min'] - test['d1_diasbp_noninvasive_min']\n",
    "test['h1_diasbp_invnoninv_min_diff'] = test['h1_diasbp_invasive_min'] - test['h1_diasbp_noninvasive_min']\n",
    "\n",
    "test['d1_sysbp_invnoninv_max_diff'] = test['d1_sysbp_invasive_max'] - test['d1_sysbp_noninvasive_max']\n",
    "test['h1_sysbp_invnoninv_max_diff'] = test['h1_sysbp_invasive_max'] - test['h1_sysbp_noninvasive_max']\n",
    "test['d1_sysbp_invnoninv_min_diff'] = test['d1_sysbp_invasive_min'] - test['d1_sysbp_noninvasive_min']\n",
    "test['h1_sysbp_invnoninv_min_diff'] = test['h1_sysbp_invasive_min'] - test['h1_sysbp_noninvasive_min']\n",
    "\n",
    "\n",
    "for v in ['albumin','bilirubin','bun','glucose','hematocrit','pao2fio2ratio','arterial_ph','resprate','sodium','temp','wbc','creatinine']:\n",
    "    train[f'{v}_indicator'] = (((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'h1_{v}_max'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'd1_{v}_min'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'h1_{v}_min'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'd1_{v}_max'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'h1_{v}_min'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'd1_{v}_min'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'd1_{v}_max'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'h1_{v}_min'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'h1_{v}_max'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'h1_{v}_max'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'd1_{v}_min'])) |\n",
    "                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'd1_{v}_max'])) \n",
    "                ).astype(np.int8)\n",
    "    test[f'{v}_indicator'] = (((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'h1_{v}_max'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'd1_{v}_min'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'h1_{v}_min'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'd1_{v}_max'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'h1_{v}_min'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'd1_{v}_min'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'd1_{v}_max'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'h1_{v}_min'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'h1_{v}_max'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'h1_{v}_max'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'd1_{v}_min'])) |\n",
    "                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'd1_{v}_max'])) \n",
    "                ).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:08:39.510568Z",
     "iopub.status.busy": "2021-02-10T13:08:39.509767Z",
     "iopub.status.idle": "2021-02-10T13:08:39.545675Z",
     "shell.execute_reply": "2021-02-10T13:08:39.544717Z"
    },
    "papermill": {
     "duration": 0.119888,
     "end_time": "2021-02-10T13:08:39.545952",
     "exception": false,
     "start_time": "2021-02-10T13:08:39.426064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "more_extreme_cols = [c for c in train.columns if(c.endswith(\"_day_more_extreme\"))]\n",
    "train[\"total_day_more_extreme\"] = train[more_extreme_cols].sum(axis=1)\n",
    "test[\"total_day_more_extreme\"] = test[more_extreme_cols].sum(axis=1)\n",
    "train[\"d1_resprate_div_mbp_min\"] = train[\"d1_resprate_min\"].div(train[\"d1_mbp_min\"])\n",
    "train[\"d1_resprate_div_sysbp_min\"] = train[\"d1_resprate_min\"].div(train[\"d1_sysbp_min\"])\n",
    "train[\"d1_lactate_min_div_diasbp_min\"] = train[\"d1_lactate_min\"].div(train[\"d1_diasbp_min\"])\n",
    "train[\"d1_heartrate_min_div_d1_sysbp_min\"] = train[\"d1_heartrate_min\"].div(train[\"d1_sysbp_min\"])\n",
    "train[\"d1_hco3_div\"]= train[\"d1_hco3_max\"].div(train[\"d1_hco3_min\"])\n",
    "train[\"d1_resprate_times_resprate\"] = train[\"d1_resprate_min\"].multiply(train[\"d1_resprate_max\"])\n",
    "train[\"left_average_spo2\"] = (2*train[\"d1_spo2_max\"] + train[\"d1_spo2_min\"])/3\n",
    "test[\"d1_resprate_div_mbp_min\"] = test[\"d1_resprate_min\"].div(test[\"d1_mbp_min\"])\n",
    "test[\"d1_resprate_div_sysbp_min\"] = test[\"d1_resprate_min\"].div(test[\"d1_sysbp_min\"])\n",
    "test[\"d1_lactate_min_div_diasbp_min\"] = test[\"d1_lactate_min\"].div(test[\"d1_diasbp_min\"])\n",
    "test[\"d1_heartrate_min_div_d1_sysbp_min\"] = test[\"d1_heartrate_min\"].div(test[\"d1_sysbp_min\"])\n",
    "test[\"d1_hco3_div\"]= test[\"d1_hco3_max\"].div(test[\"d1_hco3_min\"])\n",
    "test[\"d1_resprate_times_resprate\"] = test[\"d1_resprate_min\"].multiply(test[\"d1_resprate_max\"])\n",
    "test[\"left_average_spo2\"] = (2*test[\"d1_spo2_max\"] + test[\"d1_spo2_min\"])/3\n",
    "train[\"total_chronic\"] = train[[\"aids\",\"cirrhosis\", 'hepatic_failure']].sum(axis=1)\n",
    "train[\"total_cancer_immuno\"] = train[[ 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].sum(axis=1)\n",
    "test[\"total_chronic\"] = test[[\"aids\",\"cirrhosis\", 'hepatic_failure']].sum(axis=1)\n",
    "test[\"total_cancer_immuno\"] = test[[ 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].sum(axis=1)\n",
    "train[\"has_complicator\"] = train[[\"aids\",\"cirrhosis\", 'hepatic_failure',\n",
    "                            'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].max(axis=1)\n",
    "test[\"has_complicator\"] = test[[\"aids\",\"cirrhosis\", 'hepatic_failure',\n",
    "                            'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].max(axis=1)\n",
    "gc.collect()\n",
    "train[[\"has_complicator\",\"total_chronic\",\"total_cancer_immuno\",\"has_complicator\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['apache_3j'] = np.where(train['apache_3j_diagnosis_type']<0 , np.nan ,\n",
    "                            np.where(train['apache_3j_diagnosis_type'] < 200, 'Cardiovascular' , \n",
    "                            np.where(train['apache_3j_diagnosis_type'] < 400, 'Respiratory' , \n",
    "                            np.where(train['apache_3j_diagnosis_type'] < 500, 'Neurological' , \n",
    "                            np.where(train['apache_3j_diagnosis_type'] < 600, 'Sepsis' , \n",
    "                            np.where(train['apache_3j_diagnosis_type'] < 800, 'Trauma' ,  \n",
    "                            np.where(train['apache_3j_diagnosis_type'] < 900, 'Haematological' ,         \n",
    "                            np.where(train['apache_3j_diagnosis_type'] < 1000, 'Renal/Genitourinary' ,         \n",
    "                            np.where(train['apache_3j_diagnosis_type'] < 1200, 'Musculoskeletal/Skin disease' , 'Operative Sub-Diagnosis Codes' ))))))))\n",
    "                                    )\n",
    "test['apache_3j'] = np.where(test['apache_3j_diagnosis_type']<0 , np.nan ,\n",
    "                            np.where(test['apache_3j_diagnosis_type'] < 200, 'Cardiovascular' , \n",
    "                            np.where(test['apache_3j_diagnosis_type'] < 400, 'Respiratory' , \n",
    "                            np.where(test['apache_3j_diagnosis_type'] < 500, 'Neurological' , \n",
    "                            np.where(test['apache_3j_diagnosis_type'] < 600, 'Sepsis' , \n",
    "                            np.where(test['apache_3j_diagnosis_type'] < 800, 'Trauma' ,  \n",
    "                            np.where(test['apache_3j_diagnosis_type'] < 900, 'Haematological' ,         \n",
    "                            np.where(test['apache_3j_diagnosis_type'] < 1000, 'Renal/Genitourinary' ,         \n",
    "                            np.where(test['apache_3j_diagnosis_type'] < 1200, 'Musculoskeletal/Skin disease' , 'Operative Sub-Diagnosis Codes' ))))))))\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of FE is done in another notebook and saved in a dataset because of Memory issues. Most of Them are from [Kain's work](http://) and are two types of features: \n",
    "* data grouped by a categorical feature then mean and std of numerical columns are calculated.\n",
    "* New categorical columns created by adding two categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:08:49.912716Z",
     "iopub.status.busy": "2021-02-10T13:08:49.9119Z",
     "iopub.status.idle": "2021-02-10T13:09:01.164775Z",
     "shell.execute_reply": "2021-02-10T13:09:01.165363Z"
    },
    "papermill": {
     "duration": 11.321062,
     "end_time": "2021-02-10T13:09:01.165611",
     "exception": false,
     "start_time": "2021-02-10T13:08:49.844549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainf = pd.read_pickle('../input/featxx/X.pkl')\n",
    "testf = pd.read_pickle('../input/featxx/X_test.pkl')\n",
    "trainf = trainf.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\n",
    "testf = testf.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = train.columns.tolist()\n",
    "train = train[col_order]\n",
    "test = test[col_order]\n",
    "col_order = trainf.columns.tolist()\n",
    "trainf = trainf[col_order]\n",
    "testf = testf[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:09:01.435325Z",
     "iopub.status.busy": "2021-02-10T13:09:01.434205Z",
     "iopub.status.idle": "2021-02-10T13:09:04.917455Z",
     "shell.execute_reply": "2021-02-10T13:09:04.917922Z"
    },
    "papermill": {
     "duration": 3.555102,
     "end_time": "2021-02-10T13:09:04.918178",
     "exception": false,
     "start_time": "2021-02-10T13:09:01.363076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.concat([trainf.reset_index(drop=True),train.reset_index(drop=True)], axis=1)\n",
    "test =  pd.concat([testf.reset_index(drop=True),test.reset_index(drop=True)], axis=1)\n",
    "train= train.fillna(0); test= test.fillna(0)\n",
    "gc.collect()\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Duplicated Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:09:06.854475Z",
     "iopub.status.busy": "2021-02-10T13:09:06.853468Z",
     "iopub.status.idle": "2021-02-10T13:09:07.560871Z",
     "shell.execute_reply": "2021-02-10T13:09:07.560317Z"
    },
    "papermill": {
     "duration": 0.780758,
     "end_time": "2021-02-10T13:09:07.561054",
     "exception": false,
     "start_time": "2021-02-10T13:09:06.780296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Cols = list(train.columns)\n",
    "for i,item in enumerate(train.columns):\n",
    "    if item in train.columns[:i]: Cols[i] = \"toDROP\"\n",
    "train.columns = Cols\n",
    "test.columns = Cols\n",
    "train = train.drop(\"toDROP\",1)\n",
    "test = test.drop(\"toDROP\",1)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reordering Columns again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = train.columns.tolist()\n",
    "train = train[col_order]\n",
    "test = test[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following features are dropped with permutation importance and target permutations based on [https://www.kaggle.com/ogrellier](https://www.kaggle.com/ogrellier)'s work and some are highly correlated features or features with different distribution in train and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['abmi', 'age_type', 'aids', 'albumin_apache', 'albumin_h1_value_range', \n",
    "             'albumin_h1_zero_range','albumin_tot_change_value_range_normed', \n",
    "             'apache_3j_diagnosis-cat_age', 'apache_post_operative','apache_post_operative_std_d1_temp_max', \n",
    "             'arf_apache_std_d1_hemaglobin_max', 'arterial_pco2_d1_h1_max_eq','arterial_pco2_d1_h1_min_eq', \n",
    "             'arterial_pco2_d1_zero_range', 'arterial_pco2_h1_zero_range','arterial_ph_apache', \n",
    "             'arterial_ph_d1_h1_max_eq', 'arterial_ph_d1_value_range', 'arterial_ph_d1_zero_range',\n",
    "             'arterial_ph_h1_zero_range', 'arterial_po2_d1_h1_max_eq', 'arterial_po2_d1_h1_min_eq', \n",
    "             'arterial_po2_d1_value_range', 'bilirubin_h1_value_range', 'bilirubin_h1_zero_range',\n",
    "             'bilirubin_tot_change_value_range_normed', 'bmi_type', 'bun_d1_h1_max_eq', \n",
    "             'bun_d1_zero_range','bun_h1_value_range', 'bun_h1_zero_range', 'calcium_d1_zero_range', \n",
    "             'calcium_h1_value_range','calcium_h1_zero_range', 'creatinine_h1_zero_range', 'd1_albumin_min', \n",
    "             'd1_arterial_pco2_min','d1_arterial_ph_max', 'd1_arterial_ph_min', 'd1_calcium_max', \n",
    "             'd1_diasbp_max', 'd1_diasbp_min','d1_hematocrit_min', 'd1_inr_max', 'd1_inr_min', \n",
    "             'd1_mbp_invasive_max', 'd1_mbp_invasive_min', 'd1_mbp_max','d1_mbp_min', 'd1_mbp_noninvasive_max', \n",
    "             'd1_mbp_noninvasive_min', 'd1_pao2fio2ratio_max', 'd1_pao2fio2ratio_min', 'd1_platelets_max', \n",
    "             'd1_resprate_max', 'd1_sysbp_invasive_min', 'd1_temp_min','d1_wbc_min', 'diasbp_d1_h1_max_eq', \n",
    "             'diasbp_d1_zero_range', 'diasbp_invasive_d1_h1_max_eq','diasbp_invasive_d1_value_range', \n",
    "             'diasbp_invasive_d1_zero_range', 'diasbp_invasive_h1_value_range','diasbp_invasive_h1_zero_range', \n",
    "             'diasbp_noninvasive_d1_h1_max_eq', 'diasbp_noninvasive_d1_zero_range','diasbp_noninvasive_h1_zero_range', \n",
    "             'diff_bmi', 'elective_surgery_mean_d1_sysbp_min', 'gcs_unable_apache','h1_albumin_max', 'h1_albumin_min', \n",
    "             'h1_arterial_pco2_max', 'h1_arterial_pco2_min', 'h1_arterial_ph_min','h1_arterial_po2_max', \n",
    "             'h1_bilirubin_max', 'h1_bun_max', 'h1_creatinine_min', 'h1_diasbp_noninvasive_max','h1_heartrate_max', \n",
    "             'h1_heartrate_min', 'h1_hemaglobin_min', 'h1_hematocrit_max', 'h1_hematocrit_min','h1_lactate_max', \n",
    "             'h1_lactate_min', 'h1_mbp_invasive_max', 'h1_mbp_invasive_min', 'h1_mbp_max', 'h1_mbp_min',\n",
    "             'h1_mbp_noninvasive_max', 'h1_mbp_noninvasive_min', 'h1_pao2fio2ratio_max', 'h1_pao2fio2ratio_min',\n",
    "             'h1_platelets_max', 'h1_platelets_min', 'h1_resprate_max', 'h1_resprate_min', 'h1_sodium_max',\n",
    "             'h1_spo2_max', 'h1_spo2_min', 'h1_sysbp_max', 'h1_sysbp_min', 'h1_sysbp_noninvasive_max',\n",
    "             'h1_sysbp_noninvasive_min', 'h1_temp_max', 'h1_temp_min', 'h1_wbc_max', 'h1_wbc_min', \n",
    "             'hco3_d1_h1_max_eq','hco3_d1_h1_min_eq', 'hco3_h1_value_range', 'hco3_h1_zero_range', \n",
    "             'heartrate_d1_zero_range','heartrate_h1_zero_range', 'height', 'hemaglobin_d1_value_range', \n",
    "             'hemaglobin_d1_zero_range','hematocrit_apache', 'hematocrit_d1_h1_min_eq', \n",
    "             'hematocrit_d1_value_range', 'hematocrit_d1_zero_range','inr_d1_h1_max_eq', \n",
    "             'inr_d1_h1_min_eq', 'inr_d1_value_range', 'inr_d1_zero_range', 'inr_day_more_extreme',\n",
    "             'inr_h1_value_range', 'inr_h1_zero_range', 'inr_started_after_firstHour', \n",
    "             'intubated_apache_mean_d1_spo2_max','lactate_h1_value_range', 'lactate_h1_zero_range', \n",
    "             'lymphoma', 'map_apache', 'mbp_d1_zero_range','mbp_h1_zero_range', 'mbp_invasive_d1_h1_min_eq', \n",
    "             'mbp_invasive_d1_value_range', 'mbp_invasive_d1_zero_range','mbp_invasive_h1_zero_range', \n",
    "             'mbp_noninvasive_d1_h1_max_eq', 'mbp_noninvasive_d1_h1_min_eq','mbp_noninvasive_d1_zero_range', \n",
    "             'mbp_noninvasive_h1_zero_range', 'mean_diff_d1_inr_min','mean_diff_h1_bilirubin_min', \n",
    "             'mean_diff_h1_inr_max', 'paco2_apache', 'paco2_for_ph_apache','pao2fio2ratio_apache', \n",
    "             'pao2fio2ratio_h1_value_range', 'pao2fio2ratio_h1_zero_range','rank_frqenc_leukemia', \n",
    "             'wbc_h1_value_range','platelets_d1_value_range', 'platelets_h1_zero_range', 'potassium_d1_h1_max_eq',\n",
    "             'potassium_h1_value_range','potassium_h1_zero_range', 'rank_frqenc_apache_2_diagnosis', 'resprate_apache', \n",
    "             'resprate_d1_h1_min_eq','resprate_d1_zero_range', 'sodium_d1_h1_min_eq', 'sodium_d1_zero_range',\n",
    "             'spo2_d1_h1_max_eq','sysbp_d1_zero_range', 'sysbp_h1_zero_range', 'sysbp_invasive_d1_h1_min_eq',\n",
    "             'sysbp_invasive_d1_zero_range','sysbp_noninvasive_d1_h1_min_eq', 'sysbp_noninvasive_d1_zero_range',\n",
    "             'sysbp_noninvasive_h1_zero_range','temp_d1_zero_range', 'ventilated_apache_std_d1_glucose_min', \n",
    "             'wbc_apache', 'wbc_d1_h1_min_eq','wbc_d1_value_range', 'wbc_d1_zero_range', 'wbc_h1_zero_range', \n",
    "             'gcs_eyes_apache_mean_d1_bun_min','rank_frqenc_aids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T13:09:08.125761Z",
     "iopub.status.busy": "2021-02-10T13:09:08.124958Z",
     "iopub.status.idle": "2021-02-10T13:09:08.128149Z",
     "shell.execute_reply": "2021-02-10T13:09:08.12877Z"
    },
    "papermill": {
     "duration": 0.07394,
     "end_time": "2021-02-10T13:09:08.128998",
     "exception": false,
     "start_time": "2021-02-10T13:09:08.055058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cats = ['elective_surgery', 'icu_id', 'arf_apache', 'intubated_apache', \n",
    "        'ventilated_apache', 'cirrhosis','hepatic_failure', 'immunosuppression', \n",
    "        'leukemia', 'solid_tumor_with_metastasis', 'apache_3j_diagnosis_x',\n",
    "        'apache_2_diagnosis_x', 'apache_3j', 'apache_3j_diagnosis_split1', \n",
    "        'apache_2_diagnosis_split1', 'gcs_sum_type','hospital_admit_source', \n",
    "        'glucose_rate', 'glucose_wb', 'gcs_eyes_apache', 'glucose_normal', \n",
    "        'total_cancer_immuno','gender', 'total_chronic', 'icu_stay_type', \n",
    "        'apache_2_diagnosis_type', 'apache_3j_diagnosis_type']\n",
    "len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cats:\n",
    "    train_only = list(set(train[col].unique()) - set(test[col].unique()))\n",
    "    test_only = list(set(test[col].unique()) - set(train[col].unique()))\n",
    "    both = list(set(test[col].unique()).union(set(train[col].unique())))\n",
    "    train.loc[train[col].isin(train_only), col] = np.nan\n",
    "    test.loc[test[col].isin(test_only), col] = np.nan\n",
    "    try:\n",
    "        lbl = OrdinalEncoder(dtype='int')\n",
    "        train[col] = lbl.fit_transform(train[col].astype('str').values.reshape(-1,1))\n",
    "        test[col] = lbl.transform(test[col].astype('str').values.reshape(-1,1))\n",
    "    except:\n",
    "        lbl = OrdinalEncoder(dtype='int')\n",
    "        train[col] = lbl.fit_transform(train[col].astype('str').fillna('-1').values.reshape(-1,1))\n",
    "        test[col] = lbl.transform(test[col].astype('str').fillna('-1').values.reshape(-1,1))\n",
    "    temp = pd.concat([train[[col]], test[[col]]], axis=0)\n",
    "    temp_mapping = temp.groupby(col).size()/len(temp)\n",
    "    temp['enc'] = temp[col].map(temp_mapping)\n",
    "    temp['enc'] = stats.rankdata(temp['enc'])\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    train[f'rank_frqenc_{col}'] = temp[['enc']].values[:train.shape[0]]\n",
    "    test[f'rank_frqenc_{col}'] = temp[['enc']].values[train.shape[0]:]               \n",
    "    test[col] = test[col].astype('category')\n",
    "    train[col] = train[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = list(set(drop_cols))\n",
    "print(len(drop_cols))\n",
    "train = train.drop(drop_cols, axis=1)\n",
    "test = test.drop(drop_cols, axis=1)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame,\n",
    "                     verbose: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "\n",
    "                if (c_min > np.iinfo(np.int32).min\n",
    "                      and c_max < np.iinfo(np.int32).max):\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif (c_min > np.iinfo(np.int64).min\n",
    "                      and c_max < np.iinfo(np.int64).max):\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (c_min > np.finfo(np.float16).min\n",
    "                        and c_max < np.finfo(np.float16).max):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (c_min > np.finfo(np.float32).min\n",
    "                      and c_max < np.finfo(np.float32).max):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    reduction = (start_mem - end_mem) / start_mem\n",
    "\n",
    "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "gc.collect()\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further (generic) FE (<1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean\n",
    "from scipy.sparse import hstack, coo_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# all feature engineering functions\n",
    "def join_cols(col_ls):\n",
    "    inters = []\n",
    "    for i, col1 in enumerate(col_ls):\n",
    "        for col2 in col_ls[i+1:]:\n",
    "            inters.append(col1 + '-' + col2)\n",
    "    return inters\n",
    "\n",
    "\n",
    "def clean_data(train_df, test_df, col):\n",
    "    train_df[col] = train_df[col].astype('str')\n",
    "    test_df[col] = test_df[col].astype('str')\n",
    "\n",
    "    train_only = list(set(train_df[col].unique()) - set(test_df[col].unique()))\n",
    "    test_only = list(set(test_df[col].unique()) - set(train_df[col].unique()))\n",
    "\n",
    "    train_df.loc[train_df[col].isin(train_only), col] = np.nan\n",
    "    test_df.loc[test_df[col].isin(test_only), col] = np.nan\n",
    "    return train_df, test_df\n",
    "\n",
    "    \n",
    "def clean_normalize(train_df, test_df, col):\n",
    "    train_df[col] = train_df[col].astype('str')\n",
    "    test_df[col] = test[col].astype('str')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train_df[col], test_df[col]]))\n",
    "        \n",
    "    train_df[col] = le.transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def add_rank(train_df, test_df, col):\n",
    "    # increasing order using df.rank()\n",
    "    # try method='first'/'dense' in rank() function\n",
    "    df = pd.concat([train_df[col], test_df[col]]).rank()\n",
    "    train_df[col + '_rank'] = np.log1p(df).iloc[0:train_df.shape[0]].values\n",
    "    test_df[col + '_rank'] = np.log1p(df).iloc[train_df.shape[0]:].values\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def add_joints(train_df, test_df, col_ls):\n",
    "    inters = join_cols(col_ls)\n",
    "    \n",
    "    for f in inters:\n",
    "        col1, col2 = f.split('-')\n",
    "        train_df[f] = train_df[col1].astype('str') + '-' + train_df[col2].astype('str')\n",
    "        test_df[f] = test_df[col1].astype('str') + '-' + test_df[col2].astype('str')\n",
    "\n",
    "        train_df, test_df = clean_data(train_df, test_df, f)\n",
    "        \n",
    "        if test[f].dtype == 'object':\n",
    "            train_df, test_df = clean_normalize(train_df, test_df, f)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['age', 'bmi', 'd1_heartrate_min', 'weight']:\n",
    "    train, test = add_rank(train, test, f)\n",
    "    \n",
    "\n",
    "# categoricals, to experiment \n",
    "cols_to_join  =  ['icu_type', 'elective_surgery', 'arf_apache',\n",
    "                  'gcs_sum_type', 'intubated_apache', 'ventilated_apache', \n",
    "                  'apache_2_diagnosis_type', 'apache_3j_diagnosis']\n",
    "\n",
    "inters = join_cols(cols_to_join)\n",
    "train, test = add_joints(train, test, cols_to_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_data = lgb.Dataset(train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance and drop more columns (~10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'n_estimators': 2000,\n",
    "            'subsample': 1., \n",
    "            'subsample_freq': 0,\n",
    "            'learning_rate': 0.008,\n",
    "            'feature_fraction': 0.4428, # change accordingly\n",
    "            'max_depth': 5,\n",
    "            'lambda_l1': 6.374, # change accordingly\n",
    "            'lambda_l2': 6.05, # change accordingly\n",
    "            'scale_pos_weight': 1, # change accordingly\n",
    "            'num_leaves': 127, # change accordingly\n",
    "            'min_data_in_leaf': 48, # change accordingly\n",
    "            'verbose': -1,\n",
    "            'feature_pre_filter': False, \n",
    "            #'device': 'gpu',#'cpu', # depends on using cpu or gpu\n",
    "            'n_jobs': 4,\n",
    "            }\n",
    "\n",
    "bst = lgb.train(opt_params, lgb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_ls = bst.feature_importance()\n",
    "idx_min_fi = np.argsort(fi_ls)[:250]\n",
    "cols_min_fi = [train.columns[idx] for idx in idx_min_fi]\n",
    "train = train.drop(cols_min_fi, axis=1)\n",
    "test = test.drop(cols_min_fi, axis=1)\n",
    "\n",
    "lgb_data_dropped = lgb.Dataset(train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model before pseudo-labeling (~20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "opt_params['n_estimators'] = 5000\n",
    "bst_dropped = lgb.train(opt_params, lgb_data_dropped)\n",
    "y_hat = bst_dropped.predict(test)\n",
    "plt.hist(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bst_dropped.predict(test)\n",
    "plt.hist(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_noPL = pd.read_csv('/kaggle/input/widsdatathon2021/UnlabeledWiDS2021.csv', usecols=['encounter_id'])\n",
    "submission_noPL['diabetes_mellitus'] = y_hat\n",
    "submission_noPL.to_csv(\"submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_data(test_x, y_hat, pos_thresh, neg_thresh, pseudo_frac):\n",
    "    test_x_pseudo_1 = test_x[y_hat > pos_thresh]\n",
    "    test_x_pseudo_0 = test_x[y_hat < neg_thresh]\n",
    "\n",
    "    test_x_pseudo = np.concatenate([test_x_pseudo_1, test_x_pseudo_0])\n",
    "    y_pseudo = [1] * test_x_pseudo_1.shape[0] + [0] * test_x_pseudo_0.shape[0]\n",
    "    test_pseudo = np.concatenate([test_x_pseudo, np.expand_dims(y_pseudo, axis=1)], axis=1)\n",
    "    test_pseudo_sample = test_pseudo[np.random.choice(test_pseudo.shape[0], int(pseudo_frac * test_pseudo.shape[0]), replace=False)]\n",
    "    \n",
    "    test_pseudo_x = test_pseudo_sample[:,:-1]\n",
    "    test_pseudo_y = test_pseudo_sample[:,-1:]\n",
    "    \n",
    "    return test_pseudo_x, test_pseudo_y\n",
    "\n",
    "\n",
    "\n",
    "def train_pseudo_label(test_x, y_hat, train_x, y, pos_thresh, neg_thresh, pseudo_frac, opt_params):\n",
    "    \n",
    "    # get confident test examples\n",
    "    test_pseudo_x, test_pseudo_y = get_pseudo_data(test_x, y_hat, pos_thresh, neg_thresh, pseudo_frac)\n",
    "    \n",
    "    # add confident test examples to training data\n",
    "    train_x_pseudo = np.concatenate([train_x.values, test_pseudo_x])\n",
    "    train_y_pseudo = np.concatenate([y.values, test_pseudo_y])\n",
    "    \n",
    "    # retrain model\n",
    "    lgb_data = lgb.Dataset(train_x_pseudo, train_y_pseudo)\n",
    "    ## TODO cv\n",
    "    bst = lgb.train(opt_params, lgb_data)\n",
    "    \n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can adjust the 3 float numbers here\n",
    "bst_pl = train_pseudo_label(test, y_hat, train, y, 0.9, 0.05, 1.0, opt_params)\n",
    "y_pl = bst_pl.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_PL = pd.read_csv('/kaggle/input/widsdatathon2021/UnlabeledWiDS2021.csv', usecols=['encounter_id'])\n",
    "submission_PL['diabetes_mellitus'] = y_pl\n",
    "submission_PL.to_csv(\"submission_pl.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble the previous two results (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.mean(np.concatenate([y_hat, y], axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
